{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Borough json\n",
    "with open('../data/raw/misc/london_boroughs.json') as json_file:\n",
    "    borough_coordinates = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import commuter data\n",
    "borough_commuters = pd.read_csv(\"../data/raw/commuter/commuting-patterns-borough.csv\")\n",
    "bc = copy.deepcopy(borough_commuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Borough names and Origin/Destination dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 18823.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collect borough names\n",
    "boroughs = []\n",
    "boroughs_no_spaces = []\n",
    "for f in tqdm(borough_coordinates['features']):\n",
    "    boroughs.append(f['properties']['name'])\n",
    "    boroughs_no_spaces.append(f['properties']['name'].replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barking and Dagenham\n",
      "Barnet\n",
      "Bexley\n",
      "Brent\n",
      "Bromley\n",
      "Camden\n",
      "City of London\n",
      "Croydon\n",
      "Ealing\n",
      "Enfield\n",
      "Greenwich\n",
      "Hackney\n",
      "Hammersmith and Fulham\n",
      "Haringey\n",
      "Harrow\n",
      "Havering\n",
      "Hillingdon\n",
      "Hounslow\n",
      "Islington\n",
      "Kensington and Chelsea\n",
      "Kingston upon Thames\n",
      "Lambeth\n",
      "Lewisham\n",
      "Merton\n",
      "Newham\n",
      "Redbridge\n",
      "Richmond upon Thames\n",
      "Southwark\n",
      "Sutton\n",
      "Tower Hamlets\n",
      "Waltham Forest\n",
      "Wandsworth\n",
      "Westminster\n"
     ]
    }
   ],
   "source": [
    "for b in np.sort(boroughs):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset columns based on borough names\n",
    "bc = bc[['Origin Area']+boroughs+['Into area (A)','Out of area (B)']]\n",
    "\n",
    "# Subset rows based on borough names\n",
    "bc = bc[bc['Origin Area'].isin(boroughs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from origin names\n",
    "bc.loc[:,'Origin Area'] = bc['Origin Area'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "# Remove spaces from destination names\n",
    "bc = bc.rename(columns = dict(zip(np.sort(boroughs), np.sort(boroughs_no_spaces))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate inflow/outflow columns from rest of O/D matrix\n",
    "in_out_flows = copy.deepcopy(bc[['Origin Area','Into area (A)','Out of area (B)']])\n",
    "od_matrix = copy.deepcopy(bc.drop(columns=['Into area (A)','Out of area (B)']))\n",
    "\n",
    "# Rename columns\n",
    "in_out_flows = in_out_flows.rename(columns={'Origin Area':'Origin','Into area (A)':'Inflows','Out of area (B)':'Outflows'})\n",
    "od_matrix = od_matrix.rename(columns={'Origin Area':'Origin'})\n",
    "\n",
    "# Update index\n",
    "in_out_flows = in_out_flows.set_index(keys='Origin')\n",
    "od_matrix = od_matrix.set_index(keys='Origin')\n",
    "\n",
    "# Sort by index\n",
    "in_out_flows = in_out_flows.sort_index(axis=0)\n",
    "od_matrix = od_matrix.sort_index(axis=1).sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get origin supply\n",
    "origin_supply = copy.deepcopy(od_matrix.sum(axis=1).reset_index())\n",
    "# origin_supply = origin_supply.rename(columns={'index':'Origin',0:'Supply'})\n",
    "origin_supply.columns = ['Origin','Supply']\n",
    "origin_supply = origin_supply.set_index('Origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get destination demand\n",
    "destination_demand = copy.deepcopy(od_matrix.sum(axis=0).reset_index())\n",
    "#destination_demand = destination_demand.rename(columns={'Origin':'Destination',0:'Demand'})\n",
    "destination_demand.columns = ['Destination','Demand']\n",
    "destination_demand = destination_demand.set_index('Destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with ones\n",
    "od_matrix = od_matrix.replace(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data as dataframe and numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "in_out_flows.to_csv('../data/input/commuter/inflows_outflows.csv')\n",
    "# Export to txt\n",
    "np.savetxt('../data/input/commuter/inflows_outflows.txt',in_out_flows.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "od_matrix.to_csv('../data/input/commuter/od_matrix.csv')\n",
    "# Export to txt\n",
    "np.savetxt('../data/input/commuter/od_matrix.txt',od_matrix.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "origin_supply.to_csv('../data/input/commuter/origin_supply.csv')\n",
    "# Export to txt\n",
    "np.savetxt('../data/input/commuter/origin_supply.txt',origin_supply.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "destination_demand.to_csv('../data/input/commuter/destination_demand.csv')\n",
    "# Export to txt\n",
    "np.savetxt('../data/input/commuter/destination_demand.txt',destination_demand.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRes project",
   "language": "python",
   "name": "stdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
