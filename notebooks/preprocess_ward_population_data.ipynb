{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import processed ward coordinates\n",
    "ward_coordinates = gpd.read_file(\"../data/input/misc/ward_coordinates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ward population data\n",
    "ward_atlas_pop = pd.read_csv(\"../data/raw/misc/ward_atlas_population_estimates.csv\")\n",
    "ward_ethnic_pop = pd.read_csv(\"../data/raw/misc/ethnic-group-ward-2001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify population and coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "ward_ethnic_pop = ward_ethnic_pop.rename(columns={'Borough':'borough','Ward':'ward'}).sort_values(by=['borough','ward'])\n",
    "ward_atlas_pop = ward_atlas_pop.rename(columns={'Borough':'borough','Names':'ward'}).sort_values(by=['borough','ward'])\n",
    "\n",
    "# Choose dataset\n",
    "dataset = 'ethnic' # 'ethnic' or 'atlas'\n",
    "if dataset == 'ethnic':\n",
    "    ward_pop = copy.deepcopy(ward_ethnic_pop)\n",
    "elif dataset == 'atlas':\n",
    "    ward_pop = copy.deepcopy(ward_atlas_pop)\n",
    "else: \n",
    "    raise ValueError(f\"Dataset '{dataset}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ward and borough names\n",
    "# Create new ward and borough name columns without spaces\n",
    "def fix_str(x):\n",
    "    return x.replace(' ','').replace('.','').replace('`',\"'\")\n",
    "\n",
    "ward_pop.loc[:,'ward_new'] = ward_pop['ward'].apply(lambda x: fix_str(x))\n",
    "ward_pop.loc[:,'borough_new'] = ward_pop['borough'].apply(lambda x: x.replace(' ',''))\n",
    "\n",
    "# Duplicate ward-borough pairs\n",
    "duplicate_ward_borough = ward_pop[ward_pop['ward'].isin(ward_pop['ward'][ward_pop['ward'].duplicated()])][['ward','borough']].values\n",
    "\n",
    "# Loop over duplicates and rename them so they are unique across all boroughs\n",
    "for w, b in duplicate_ward_borough:\n",
    "    ward_pop.loc[(ward_pop.ward==w) & (ward_pop.borough==b),'ward_new'] = fix_str(str(w))+'_'+str(b.replace(' ',''))\n",
    "\n",
    "weird_wards = ['HamPetershamandRichmondRiverside','StMargarets&NorthTwickenham']\n",
    "ward_pop.loc[ward_pop.ward_new==weird_wards[0],'ward_new'] = \"Ham\" \n",
    "ward_pop.loc[ward_pop.ward_new==weird_wards[1],'ward_new'] = \"StMargaretsandNorthTwickenham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute set difference of json wards - csv wards\n",
    "# For consistency this has to be empty\n",
    "assert len(np.sort(list(set(list(ward_coordinates.ward_new)) - set(list(ward_pop.ward_new))))) == 0\n",
    "# Compute set difference of csv wards - json wards\n",
    "# For consistency this has to be empty\n",
    "assert len(np.sort(list(set(list(ward_pop.ward_new)) - set(list(ward_coordinates.ward_new))))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge population with coordinate dataframes\n",
    "ward_population = pd.merge(ward_coordinates, ward_pop, on=['ward_new', 'borough_new']).sort_values('ward_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get origin supply\n",
    "origin_supply = copy.deepcopy(ward_population[['ward_new','pop2001','lon','lat']])\n",
    "# origin_supply = origin_supply.rename(columns={'index':'Origin',0:'Supply'})\n",
    "origin_supply.columns = ['origin','supply','lon','lat']\n",
    "origin_supply = origin_supply.set_index('origin').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data as dataframe and numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628, 3)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_supply.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "origin_supply.to_csv('../data/input/commuter/origin_supply.csv')\n",
    "# Export to txt\n",
    "np.savetxt('../data/input/commuter/origin_supply.txt',origin_supply['supply'].to_numpy())\n",
    "np.savetxt('../data/input/commuter/origin_locations.txt',origin_supply[['lon','lat']].to_numpy())\n",
    "np.savetxt('../data/input/commuter/origins.txt',origin_supply.index.to_numpy(),fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRes project",
   "language": "python",
   "name": "stdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
